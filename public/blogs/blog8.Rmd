---
categories:
- ""
- ""
date: "2017-10-31T21:28:43-05:00"
description: ""
draft: false
image: pic02.jpg
keywords: ""
slug: blog8
title: Analyzing aspects of the North American Society
---

```{r load-libraries, include=FALSE}
library(tidyverse)  
library(mosaic)
library(ggthemes)
library(lubridate)
library(here)
library(skimr)
library(janitor)
library(httr)
library(readxl)
library(vroom)
library(infer)
```

I will analyze data from the **2016 GSS sample data**, to estimate values of *population parameters* of interest about US adults. The GSS sample data file has 2867 observations of 935 variables, but we are only interested in very few of these variables and you are using a smaller file.

The [General Social Survey (GSS)](http://www.gss.norc.org/) gathers data on American society in order to monitor and explain trends in attitudes, behaviours, and attributes. Many trends have been tracked for decades, so one can see the evolution of attitudes, etc in American Society.


```{r, read_gss_data, cache=TRUE}
gss <- read_csv(here::here("static", "data", "smallgss2016.csv"), 
                na = c("", "Don't know",
                       "No answer", "Not applicable"))
```

We notice that many responses should not be taken into consideration, like "No Answer", "Don't Know", "Not applicable", "Refused to Answer".

We will be creating 95% confidence intervals for population parameters. The variables we have are the following:

- hours and minutes spent on email weekly. The responses to these questions are recorded in the `emailhr` and `emailmin` variables. For example, if the response is 2.50 hours, this would be recorded as emailhr = 2 and emailmin = 30.
- `snapchat`, `instagrm`, `twitter`: whether respondents used these social media in 2016
- `sex`: Female - Male
- `degree`: highest education level attained

## Instagram and Snapchat, by sex

We want to estimate the *population* proportion of Snapchat or Instagram users in 2016.

We can do this, but in order to respond to this question, we have to solve for certain variables and perform calculations, as presented below. 

We create a  new variable, `snap_insta` that is *Yes* if the respondent reported using any of Snapchat (`snapchat`) or Instagram (`instagrm`), and *No* if not. If the recorded value was NA for both of these questions, the value in your new variable should also be NA.

```{r, nap_insta, cache=TRUE}

gss2 <- gss %>% 
mutate(snap_insta = case_when(snapchat == "Yes" ~ "Yes", 
                              instagrm == "Yes" ~ "Yes", is.na(snapchat) & 
                                is.na(instagrm) ~ "NA_character_",
                              snapchat == "No" & instagrm == "No" ~ "No"))               
  
```

Then we calculated the proportion of Yes’s for `snap_insta` among those who answered the question, i.e. excluding NAs.

```{r, nap_insta2, cache=TRUE}

proportion_users <- gss2 %>% 
  filter(!is.na(snap_insta)) %>% 
  summarise( 
    InstaSnap_Users = count(snap_insta == "Yes"),
    Non_Users = count(snap_insta == "No"),
    Proportion = InstaSnap_Users/ (InstaSnap_Users + Non_Users)
    )

proportion_users
  
```

Using the CI formula for proportions, we constructed 95% CIs for men and women who used either Snapchat or Instagram

```{r, nap_insta3, cache=TRUE}

proportion_users_gender <- gss2 %>% 
  filter(!is.na(snap_insta)) %>% 
  group_by(sex) %>% 
  summarise(
    InstaSnap_Users = count(snap_insta == "Yes"),
    Non_Users = count(snap_insta == "No"),
    Count = n(),
    Proportion = InstaSnap_Users/ (InstaSnap_Users + Non_Users)
    )

proportion_users_gender  # proportion - proportion_yes / count of original value

proportions_gender_ci <- proportion_users_gender %>% 
  summarise(
    proportion = Proportion,
    size = Count,
    SE = sqrt((proportion*(1-proportion))/size),
    tcritical = qt(0.975, size-1),
    lower_ci = proportion - tcritical*SE,
    upper_ci = proportion + tcritical*SE
  )

proportions_gender_ci
  
```


## Twitter, by education level

Next we want to estimate the *population* proportion of Twitter users by education level in 2016?. 

There are 5 education levels in variable `degree` which, in ascneding order of years of education, are Lt high school, High School, Junior college, Bachelor, Graduate. 

At first we turn `degree` from a character variable into a factor variable. We made sure that the order is the correct one and that levels are not sorted alphabetically which is what R by default does. 

```{r, nap_twitter, cache=TRUE}

factor_degree<- gss %>% 
  na.omit(degree) %>% 
  mutate(degree = factor(degree, 
                         levels = c("Lt high school", 
                                    "High School", 
                                    "Junior college", 
                                    "Bachelor", 
                                    "Graduate"))) %>% 
  arrange((degree))

factor_degree

```

Next, we create a  new variable, `bachelor_graduate` that is *Yes* if the respondent has either a `Bachelor` or `Graduate` degree. As before, if the recorded value for either was NA, the value in your new variable should also be NA.

```{r, nap_twitter2, cache=TRUE}

bachelor_graduate <-factor_degree%>%
  mutate(bachelor_graduate = case_when(
    degree == "Bachelor" ~ "Yes",
    degree == "Graduate" ~ "Yes",
    degree != "Graduate"& degree != "Bachelor" ~ "No",
    TRUE ~ "NA"
  ))

view(bachelor_graduate)
```

We calculate the proportion of `bachelor_graduate` who do (Yes) and who don't (No) use twitter. 

```{r, nap_twitter3, cache=TRUE}

proportion_twitter_bachelor_graduate <- bachelor_graduate %>% 
  filter(bachelor_graduate =="Yes") %>% 
  count(twitter)%>% 
  pivot_wider(names_from=twitter , values_from=n) %>% 
  mutate(twitter_yes= Yes/(No+Yes), twitter_no= No/(Yes+No))

proportion_twitter_bachelor_graduate
  
```

Using the CI formula for proportions, we constructed two 95% CIs for `bachelor_graduate` vs whether they use (Yes) and don't (No) use twitter.

```{r, nap_twitter4, cache=TRUE}

confidential_interval_twitter_yes <- proportion_twitter_bachelor_graduate %>% 
  mutate(SE=sqrt((twitter_yes*(1-twitter_yes)/(Yes+No)))) %>% 
  summarise(lower_95=twitter_yes-1.96*SE, 
            upper_95=twitter_yes+1.96*SE) %>% 
  summarise(CI_Yes_Twitter=c(lower_95,upper_95))

confidential_interval_twitter_yes

confidential_interval_twitter_no <- proportion_twitter_bachelor_graduate %>% 
  mutate(SE=sqrt((twitter_no*(1-twitter_no)/(Yes+No)))) %>% 
  summarise(lower_95=twitter_no-1.96*SE, 
            upper_95=twitter_no+1.96*SE) %>% 
  summarise(CI_No_Twitter=c(lower_95,upper_95))

confidential_interval_twitter_no

```

Do these two Confidence Intervals overlap?

No, the two Conidence Intervals do not overlap. That means that the difference between Bachelor graduates who use (YES) and do not use (NO) is not statistically significant.


## Email usage

At last, we estimate the *population* parameter on time spent on email weekly.

First we create a new variable called `email` that combines `emailhr` and `emailmin` to reports the number of minutes the respondents spend on email weekly.

```{r, nap_email1, cache=TRUE}

email <- gss %>%
  mutate_at(vars(emailhr,emailmin), funs(as.numeric)) %>% 
  mutate(email=(emailhr*60)+emailmin) %>% 
  arrange(desc(email))

email

```

We want to visualize the distribution of this new variable. We also find the mean and the median number of minutes respondents spend on email weekly. 

```{r, nap_email2, cache=TRUE}

summary_email <- email %>%
   na.omit(email) %>%
  summarize(mean = mean(email), 
            median = median(email)) 

summary_email

ggplot(email, aes(y=email)) +
  geom_boxplot()+
  scale_y_log10() +
  theme_bw()+
  labs(title="US Adults spend more than 100 Minutes on Emails a Week!",
       y="Email Minutes"
       )

```

3. Is the mean or the median a better measure of the typical amoung of time Americans spend on email weekly? Why?

Whenever a graph falls on a normal distribution, using the mean is a good choice. But if your data has extreme scores (such as the difference between a someone spends 6000 minutes and someone who spends 100 minutes at email a week), you will need to look at median, because we find a much more representative number for our data.

Using the `infer` package, we calculated a 95% bootstrap confidence interval for the mean amount of time Americans spend on email weekly. We interpreted this interval in context of the data, reporting its endpoints in “humanized” units (e.g. instead of 108 minutes, report 1 hr and 8 minutes).

```{r, nap_email3, cache=TRUE}

bootstrap_email_ci <- email %>% 
  specify(response = email) %>% 
  generate(reps = 100, type="bootstrap") %>% 
  calculate(stat = "mean") %>% 
  get_confidence_interval(level = 0.95, type = "percentile") %>% 
  mutate(lower_ci = lower_ci/60, upper_ci = upper_ci/60) # transform from min into h + min as decimal

bootstrap_email_ci

```

The confidence interval for the mean email usage in the population is between 6hrs 24min and 7hrs 28min. This seems like a reasonable interval given that the sample mean is 6hrs and 57min.

4. Would you expect a 99% confidence interval to be wider or narrower than the interval you calculated above? Explain your reasoning.

If we were to increase the confidence level of the interval to 99%, then the range of the interval would increase. Increasing the confidence will increase the margin of error resulting in a wider interval. In general, the greater the confidence level, the greater the range between the high and low points in a confidence interval.